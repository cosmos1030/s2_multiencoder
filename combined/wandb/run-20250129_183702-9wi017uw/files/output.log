
[INFO] Resume from checkpoint: /datasets/external/s2_multiencoder/checkpoints/linear_probing_multi_s2/checkpoints/best_epoch_1.pth
Traceback (most recent call last):
  File "/notebooks/s2_multiencoder/combined/main.py", line 108, in <module>
    main()
  File "/notebooks/s2_multiencoder/combined/main.py", line 69, in main
    best_ckpt = train_dual_encoder_probe(
                ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/notebooks/s2_multiencoder/combined/train.py", line 42, in train_dual_encoder_probe
    model.load_state_dict(checkpoint["model_state_dict"])
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DualEncoderLinearProbe:
	Unexpected key(s) in state_dict: "clip_model.vision_model.embeddings.class_embedding", "clip_model.vision_model.embeddings.patch_embedding.weight", "clip_model.vision_model.embeddings.position_embedding.weight", "clip_model.vision_model.pre_layrnorm.weight", "clip_model.vision_model.pre_layrnorm.bias", "clip_model.vision_model.encoder.layers.0.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.0.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.0.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.0.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.0.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.0.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.0.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.0.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.0.layer_norm1.weight", "clip_model.vision_model.encoder.layers.0.layer_norm1.bias", "clip_model.vision_model.encoder.layers.0.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.0.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.0.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.0.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.0.layer_norm2.weight", "clip_model.vision_model.encoder.layers.0.layer_norm2.bias", "clip_model.vision_model.encoder.layers.1.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.1.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.1.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.1.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.1.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.1.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.1.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.1.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.1.layer_norm1.weight", "clip_model.vision_model.encoder.layers.1.layer_norm1.bias", "clip_model.vision_model.encoder.layers.1.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.1.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.1.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.1.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.1.layer_norm2.weight", "clip_model.vision_model.encoder.layers.1.layer_norm2.bias", "clip_model.vision_model.encoder.layers.2.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.2.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.2.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.2.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.2.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.2.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.2.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.2.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.2.layer_norm1.weight", "clip_model.vision_model.encoder.layers.2.layer_norm1.bias", "clip_model.vision_model.encoder.layers.2.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.2.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.2.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.2.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.2.layer_norm2.weight", "clip_model.vision_model.encoder.layers.2.layer_norm2.bias", "clip_model.vision_model.encoder.layers.3.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.3.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.3.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.3.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.3.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.3.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.3.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.3.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.3.layer_norm1.weight", "clip_model.vision_model.encoder.layers.3.layer_norm1.bias", "clip_model.vision_model.encoder.layers.3.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.3.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.3.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.3.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.3.layer_norm2.weight", "clip_model.vision_model.encoder.layers.3.layer_norm2.bias", "clip_model.vision_model.encoder.layers.4.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.4.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.4.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.4.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.4.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.4.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.4.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.4.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.4.layer_norm1.weight", "clip_model.vision_model.encoder.layers.4.layer_norm1.bias", "clip_model.vision_model.encoder.layers.4.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.4.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.4.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.4.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.4.layer_norm2.weight", "clip_model.vision_model.encoder.layers.4.layer_norm2.bias", "clip_model.vision_model.encoder.layers.5.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.5.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.5.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.5.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.5.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.5.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.5.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.5.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.5.layer_norm1.weight", "clip_model.vision_model.encoder.layers.5.layer_norm1.bias", "clip_model.vision_model.encoder.layers.5.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.5.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.5.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.5.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.5.layer_norm2.weight", "clip_model.vision_model.encoder.layers.5.layer_norm2.bias", "clip_model.vision_model.encoder.layers.6.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.6.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.6.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.6.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.6.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.6.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.6.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.6.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.6.layer_norm1.weight", "clip_model.vision_model.encoder.layers.6.layer_norm1.bias", "clip_model.vision_model.encoder.layers.6.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.6.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.6.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.6.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.6.layer_norm2.weight", "clip_model.vision_model.encoder.layers.6.layer_norm2.bias", "clip_model.vision_model.encoder.layers.7.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.7.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.7.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.7.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.7.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.7.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.7.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.7.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.7.layer_norm1.weight", "clip_model.vision_model.encoder.layers.7.layer_norm1.bias", "clip_model.vision_model.encoder.layers.7.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.7.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.7.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.7.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.7.layer_norm2.weight", "clip_model.vision_model.encoder.layers.7.layer_norm2.bias", "clip_model.vision_model.encoder.layers.8.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.8.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.8.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.8.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.8.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.8.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.8.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.8.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.8.layer_norm1.weight", "clip_model.vision_model.encoder.layers.8.layer_norm1.bias", "clip_model.vision_model.encoder.layers.8.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.8.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.8.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.8.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.8.layer_norm2.weight", "clip_model.vision_model.encoder.layers.8.layer_norm2.bias", "clip_model.vision_model.encoder.layers.9.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.9.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.9.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.9.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.9.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.9.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.9.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.9.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.9.layer_norm1.weight", "clip_model.vision_model.encoder.layers.9.layer_norm1.bias", "clip_model.vision_model.encoder.layers.9.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.9.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.9.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.9.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.9.layer_norm2.weight", "clip_model.vision_model.encoder.layers.9.layer_norm2.bias", "clip_model.vision_model.encoder.layers.10.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.10.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.10.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.10.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.10.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.10.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.10.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.10.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.10.layer_norm1.weight", "clip_model.vision_model.encoder.layers.10.layer_norm1.bias", "clip_model.vision_model.encoder.layers.10.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.10.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.10.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.10.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.10.layer_norm2.weight", "clip_model.vision_model.encoder.layers.10.layer_norm2.bias", "clip_model.vision_model.encoder.layers.11.self_attn.k_proj.weight", "clip_model.vision_model.encoder.layers.11.self_attn.k_proj.bias", "clip_model.vision_model.encoder.layers.11.self_attn.v_proj.weight", "clip_model.vision_model.encoder.layers.11.self_attn.v_proj.bias", "clip_model.vision_model.encoder.layers.11.self_attn.q_proj.weight", "clip_model.vision_model.encoder.layers.11.self_attn.q_proj.bias", "clip_model.vision_model.encoder.layers.11.self_attn.out_proj.weight", "clip_model.vision_model.encoder.layers.11.self_attn.out_proj.bias", "clip_model.vision_model.encoder.layers.11.layer_norm1.weight", "clip_model.vision_model.encoder.layers.11.layer_norm1.bias", "clip_model.vision_model.encoder.layers.11.mlp.fc1.weight", "clip_model.vision_model.encoder.layers.11.mlp.fc1.bias", "clip_model.vision_model.encoder.layers.11.mlp.fc2.weight", "clip_model.vision_model.encoder.layers.11.mlp.fc2.bias", "clip_model.vision_model.encoder.layers.11.layer_norm2.weight", "clip_model.vision_model.encoder.layers.11.layer_norm2.bias", "clip_model.vision_model.post_layernorm.weight", "clip_model.vision_model.post_layernorm.bias".
	size mismatch for linear.weight: copying a param with shape torch.Size([100, 1536]) from checkpoint, the shape in current model is torch.Size([100, 768]).